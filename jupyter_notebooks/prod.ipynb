{
	"metadata": {
		"vscode": {
			"interpreter": {
				"hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
			}
		},
		"kernelspec": {
			"name": "glue_pyspark",
			"display_name": "Glue PySpark",
			"language": "python"
		},
		"language_info": {
			"name": "Python_Glue_Session",
			"mimetype": "text/x-python",
			"codemirror_mode": {
				"name": "python",
				"version": 3
			},
			"pygments_lexer": "python3",
			"file_extension": ".py"
		}
	},
	"nbformat_minor": 4,
	"nbformat": 4,
	"cells": [
		{
			"cell_type": "markdown",
			"source": "# 1. Downloading dependencies",
			"metadata": {}
		},
		{
			"cell_type": "code",
			"source": "import sys\nimport boto3\nimport logging\nfrom awsglue.job import Job\nfrom awsglue.transforms import *\nfrom pyspark import SparkContext\nfrom pyspark.sql import SparkSession\nfrom awsglue.context import GlueContext\nfrom awsglue.utils import getResolvedOptions\nfrom awsglue.dynamicframe import DynamicFrame",
			"metadata": {
				"trusted": true
			},
			"execution_count": null,
			"outputs": []
		},
		{
			"cell_type": "code",
			"source": "logger = logging.getLogger()\nlogger.setLevel(logging.INFO)",
			"metadata": {},
			"execution_count": null,
			"outputs": []
		},
		{
			"cell_type": "markdown",
			"source": "## 1.1 Creating GlueContext and loading data",
			"metadata": {}
		},
		{
			"cell_type": "markdown",
			"source": "Firstly I'm creating a Spark Dataframe to convert in a Glue Dynamic DF later.",
			"metadata": {}
		},
		{
			"cell_type": "code",
			"source": "spark = SparkSession.builder.master(\"local[*]\").appName(\"trips_data\").getOrCreate()",
			"metadata": {},
			"execution_count": null,
			"outputs": []
		},
		{
			"cell_type": "code",
			"source": "spark",
			"metadata": {},
			"execution_count": null,
			"outputs": []
		},
		{
			"cell_type": "code",
			"source": "glue_context = GlueContext(spark.sparkContext)",
			"metadata": {},
			"execution_count": null,
			"outputs": []
		},
		{
			"cell_type": "code",
			"source": "## params: [JOB_NAME]\nargs = getResolvedOptions(sys.argv, ['JOB_NAME'])\njob = Job(glue_context)\njob.init(args['JOB_NAME'], args)",
			"metadata": {},
			"execution_count": null,
			"outputs": []
		},
		{
			"cell_type": "code",
			"source": "#df = spark.read.parquet(\"s3://903442739132-source-bucket-trips-data-01/*/*\")",
			"metadata": {},
			"execution_count": null,
			"outputs": []
		},
		{
			"cell_type": "code",
			"source": "df_dynamic = glue_context.create_dynamic_frame_from_options('s3',connection_options={'paths':['s3://903442739132-source-bucket-trips-data-01/*/*'],},format=\"parquet\",transformation_ctx = \"dynamic_frame0\")",
			"metadata": {},
			"execution_count": null,
			"outputs": []
		},
		{
			"cell_type": "code",
			"source": "logger.info(f'printSchema: {df_dynamic.printSchema()}')",
			"metadata": {},
			"execution_count": null,
			"outputs": []
		},
		{
			"cell_type": "markdown",
			"source": "# 2. Separating files by type of license",
			"metadata": {}
		},
		{
			"cell_type": "code",
			"source": "from pyspark.sql.functions import col",
			"metadata": {},
			"execution_count": null,
			"outputs": []
		},
		{
			"cell_type": "code",
			"source": "df_uber = df_dynamic.filter(f=lambda x: x[\"hvfhs_license_num\"] in \"HV0003\")",
			"metadata": {},
			"execution_count": null,
			"outputs": []
		},
		{
			"cell_type": "code",
			"source": "connection_options = {\"path\": \"s3://903442739132-type-of-license-bucket/year=2021/*\"}",
			"metadata": {},
			"execution_count": null,
			"outputs": []
		},
		{
			"cell_type": "code",
			"source": "glue_context.write_dynamic_frame.from_options(\n    frame=df_uber,\n    connection_type='s3',\n    connection_options={\n        'path': 's3://903442739132-type-of-license-bucket/year=2021/',\n    },\n    format='csv',\n    format_options={\n        'separator': \",\"\n        # ...other kwargs\n    }\n)",
			"metadata": {},
			"execution_count": null,
			"outputs": []
		}
	]
}